{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92733c3-79e0-4329-b426-6a86a0418cfb",
   "metadata": {},
   "source": [
    "# Depth Sensing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae791f4-40fa-41e3-a0a2-b7126c0a29ae",
   "metadata": {},
   "source": [
    "### iPhone LiDAR\n",
    "\n",
    "All iPhones from iPhone 12 upwards have built in built in LiDAR. This technology is used in facial recognition scanning and to control depth-based filters when editing photographs, for example to add blur. \n",
    "\n",
    "LiDAR scans produce a dense point cloud that can be exported for processing to generate volumetric data from which an accurate three-dimensional representation of the environment can be determined. This can be used by a robot to aid navigation and by a Vision Language Model to extract useful semantic information from the scene such as identifying key objects and locating targets.\n",
    "\n",
    "### Room Plan\n",
    "\n",
    "The Room Plan iPhone application converts iPhone LiDAR data to a room layout like this:\n",
    "\n",
    "![Room Plan Scan](../assets/RoomPlan.png)\n",
    "\n",
    "The Room Plan layout shows the locations of walls, windows and doors accurately located along with bounding boxes for furniture. This layout is useful as a first pass to uniquely identify large objects and to give a broad approximation of the parts of the floor that are potentially navigable.\n",
    "\n",
    "Combining the Room Plan layout with raw LiDAR point cloud data aids allocating points to unique objects. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpt-lab)",
   "language": "python",
   "name": "gpt-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
